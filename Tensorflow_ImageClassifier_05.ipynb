{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_ImageClassifier_05.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrk943iGhS+uG/dSfHiulf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Improve computer vision accuracy with convolutions\n",
        "You now know how to do fashion image recognition using a Deep Neural Network (DNN) containing three layers— the input layer (in the shape of the input data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with several parameters that influence the final accuracy, such as different sizes of hidden layers and number of training epochs.\n",
        "\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end."
      ],
      "metadata": {
        "id": "3VYZZ7USlk2I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kMskbOiBlYin"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "( train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXwNS_yvmVq1",
        "outputId": "e341b78b-20fa-401f-c4f6-1b8941a703c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images/255.0\n",
        "test_images = test_images/255.0"
      ],
      "metadata": {
        "id": "frLAaJfrmkrm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Conv2D, MaxPool2D, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation= 'relu'))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n"
      ],
      "metadata": {
        "id": "l0d8GyiRmw0z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "GV0zrMaynmvC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42UwW5t-n28A",
        "outputId": "8e73ee38-cd5d-4944-fc76-f23474abc71d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 3ms/step - loss: 0.4976 - accuracy: 0.8265\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3788 - accuracy: 0.8643\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3389 - accuracy: 0.8767\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3144 - accuracy: 0.8854\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2960 - accuracy: 0.8911\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2802 - accuracy: 0.8962\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2682 - accuracy: 0.9013\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2572 - accuracy: 0.9049\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2483 - accuracy: 0.9089\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2371 - accuracy: 0.9120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9ed208c7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images,test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaDHc3Z3n8uX",
        "outputId": "f19ba148-fd6a-49d2-b442-d2e930126536"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3488 - accuracy: 0.8777\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34880882501602173, 0.8776999711990356]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your accuracy is probably about 89% on training and 87% on validation. You can make that even better using convolutions, which narrows down the content of the image to focus on specific, distinct details.\n",
        "\n",
        "If you've ever done image processing using a filter, then convolutions will look very familiar.\n",
        "\n",
        "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can perform operations like edge detection. For example, typically a 3x3 is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has its edges enhanced.\n",
        "\n",
        "This is perfect for computer vision, because enhancing features like edges helps the computer distinguish one item from another. Better still, the amount of information needed is much less, because you'll train only on the highlighted features.\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers becomes more focused and possibly more accurate."
      ],
      "metadata": {
        "id": "8rI3qogmKueF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the code\n",
        "Run the following code. It's the same neural network as earlier, but this time with convolutional layers added first. It will take longer, but look at the impact on the accuracy:\n"
      ],
      "metadata": {
        "id": "Zr4OxrfqKuPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ElW7yLmgLWvB",
        "outputId": "6c8a1012-4d8f-4a72-d38f-a2e9b7ae150d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "YYcqghC6LlCv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since all the images are have same dimension \n",
        "# so here its not neccessary to reshape the images but for a safe side, we do reshaping\n",
        "\n",
        "# no of train images\n",
        "print(train_images.shape)\n",
        "\n",
        "# no of test images\n",
        "print(test_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg3JLGvtMLUe",
        "outputId": "5233e2bb-722c-4582-a31f-d6cd590f3458"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gather the data\n",
        "The first step is to gather the data.\n",
        "\n",
        "You'll notice that there's a change here and the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, you have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do that, then you'll get an error when training because the convolutions do not recognize the shape."
      ],
      "metadata": {
        "id": "IqJIjnxhRCRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape(60000, 28, 28,1)\n",
        "test_images = test_images.reshape(10000, 28, 28,1)\n",
        "\n",
        "# normalisation \n",
        "train_images = train_images/255.0\n",
        "test_images = test_images/255.0"
      ],
      "metadata": {
        "id": "pGXhM05MMLKE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model\n",
        "Next, define your model. Instead of the input layer at the top, you're going to add a convolutional layer. The parameters are:\n",
        "\n",
        "* The number of convolutions you want to generate. A value like 32 is a good starting point.\n",
        "* The size of the convolutional matrix, in this case a 3x3 grid.\n",
        "* The activation function to use, in this case use relu.\n",
        "* In the first layer, the shape of the input data.\n",
        "\n",
        "You'll follow the convolution with a max pooling layer, which is designed to compress the image while maintaining the content of the features that were highlighted by the convolution. By specifying (2,2) for the max pooling, the effect is to reduce the size of the image by a factor of 4. It creates a 2x2 array of pixels and picks the largest pixel value, turning 4 pixels into 1. It repeats this computation across the image, and in so doing halves the number of horizontal pixels and halves the number of vertical pixels.\n",
        "\n",
        "You can call model.summary() to see the size and shape of the network. Notice that after every max pooling layer, the image size is reduced in the following way:"
      ],
      "metadata": {
        "id": "C3mXNr8nRXY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model creation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64,(3,3),activation = 'relu',input_shape = (28,28,1)))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(10,activation='softmax'))"
      ],
      "metadata": {
        "id": "0usJbSSvMK9J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jlgKCnKRlV0",
        "outputId": "cbf72067-4feb-4f5f-db25-e3d3f11b9edb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile and train the model\n",
        "Compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set."
      ],
      "metadata": {
        "id": "HeqVU_Rf2mHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DNzOeBnQOYn4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "model.fit(train_images,train_labels,epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuPiTqRJOYg2",
        "outputId": "65b3cb0f-09e8-4b0b-c5d9-f177a01622db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 15s 3ms/step - loss: 0.4363 - accuracy: 0.8412\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2934 - accuracy: 0.8926\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2461 - accuracy: 0.9084\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2164 - accuracy: 0.9194\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1896 - accuracy: 0.9287\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1653 - accuracy: 0.9382\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1445 - accuracy: 0.9449\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1281 - accuracy: 0.9520\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1129 - accuracy: 0.9570\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0986 - accuracy: 0.9622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9ebdfa2cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images,test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW9ms9atPWEl",
        "outputId": "46abbb98-a12f-4de6-985f-f6afd64fb919"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2904 - accuracy: 0.9139\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29041293263435364, 0.9139000177383423]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's likely gone up to about 93% on the training data and 91% on the validation data.\n",
        "\n",
        "Now try running it for more epochs—say about 20—and explore the results. While the training results might seem really good, the validation results may actually go down due to a phenomenon called overfitting.\n",
        "\n",
        "Overfitting occurs when the network learns the data from the training set too well, so it's specialised to recognize only that data, and as a result is less effective at seeing other data in more general situations. For example, if you trained only on heels, then the network might be very good at identifying heels, but sneakers might confuse it.\n",
        "\n",
        "Look at the code again, and see step-by-step how the convolutions were built."
      ],
      "metadata": {
        "id": "KvoXUm8PKuHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the convolutions and pooling\n",
        "This code shows you the convolutions graphically. The print (test_labels[:100]) shows the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Take a look at the result of running the convolution on each and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less information, and it's perhaps finding a commonality between shoes based on that convolution and pooling combination."
      ],
      "metadata": {
        "id": "gMT_PVI12xoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X2EUqlIog3Q",
        "outputId": "fb8f44b5-48c8-4b79-dda7-f4853e06b61e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2904 - accuracy: 0.9139\n",
            "Test loss: 0.29041293263435364, Test accuracy: 91.39000177383423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jKCbzsxQhDw",
        "outputId": "4ba420be-6586-4d81-d394-89d2372d411e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can select some of the corresponding images for those labels and render what they look like going through the convolutions. So, in the following code, FIRST_IMAGE, SECOND_IMAGE and THIRD_IMAGE are all the indexes for value 9, an ankle boot.\n",
        "\n",
        "\n",
        "And you should see something like the following, where the convolution is taking the essence of the sole of the shoe, effectively spotting that as a common feature across all shoes."
      ],
      "metadata": {
        "id": "Hu8WwghZopbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=23\n",
        "THIRD_IMAGE=28\n",
        "CONVOLUTION_NUMBER = 6\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "pXBV4NDq4WZk",
        "outputId": "487c8229-ca14-4fd8-dfd3-645f36f516b3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "### Exercise 1\n",
        "Try editing the convolutions. Change the number of convolutions from 32 to either 16 or 64. What impact does that have on accuracy and training time?\n"
      ],
      "metadata": {
        "id": "JfxaMj314xwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtQSMTVn4WM8",
        "outputId": "66af45db-2b22-4869-d030-8a3642c2b673"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 11, 11, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 800)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               102528    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 113,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4752 - accuracy: 0.8276\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3221 - accuracy: 0.8826\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2766 - accuracy: 0.8974\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2445 - accuracy: 0.9098\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2213 - accuracy: 0.9178\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2695 - accuracy: 0.9021\n",
            "Test loss: 0.2694574296474457, Test accuracy: 90.21000266075134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gam8d3xM4WDJ",
        "outputId": "58649172-b585-42ef-9443-29ba2ab3e580"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 13, 13, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4425 - accuracy: 0.8390\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2966 - accuracy: 0.8903\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2498 - accuracy: 0.9081\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2173 - accuracy: 0.9187\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1909 - accuracy: 0.9289\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2726 - accuracy: 0.9018\n",
            "Test loss: 0.27259552478790283, Test accuracy: 90.17999768257141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63-AgpXt5DIR",
        "outputId": "6199b871-f8c9-49bc-8561-ae368932ac0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 13, 13, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 11, 11, 16)        2320      \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               51328     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 55,098\n",
            "Trainable params: 55,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5362 - accuracy: 0.8056\n",
            "Epoch 2/5\n",
            "1761/1875 [===========================>..] - ETA: 0s - loss: 0.3618 - accuracy: 0.8691"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 2\n",
        "Remove the final convolution. What impact does that have on accuracy or training time?"
      ],
      "metadata": {
        "id": "8iH5tzYv5NjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
      ],
      "metadata": {
        "id": "oSplw0fY5M9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def mat(n):\n",
        "  arr = []\n",
        "  print(arr)\n",
        "  for i in range(n):\n",
        "    a = []\n",
        "    for j in range(n):\n",
        "      a.append(random.randint(1,100))\n",
        "    arr.append(a)\n",
        "  return arr\n",
        "\n",
        "n = int(input(\"Enter : \"))\n",
        "print(mat(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLdpsmGSooMH",
        "outputId": "ec659cec-2d11-4247-8682-2133c6529f78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter : 8\n",
            "[]\n",
            "[[95, 40, 19, 75, 22, 95, 25, 32], [29, 96, 38, 34, 10, 36, 26, 28], [100, 78, 81, 42, 30, 23, 7, 16], [78, 91, 56, 20, 70, 82, 32, 67], [91, 67, 91, 71, 92, 35, 84, 65], [73, 46, 40, 36, 38, 21, 77, 26], [99, 79, 80, 15, 49, 75, 72, 43], [99, 19, 68, 100, 65, 88, 92, 43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy \n",
        "\n",
        "numpy.random.randint(1,100,(5,5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p4WGESjQrLk",
        "outputId": "bba142fa-2509-4c44-f1c6-7f84581313a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11, 34, 16, 93, 47],\n",
              "       [40, 69, 75, 54, 87],\n",
              "       [84, 86, 32, 48, 85],\n",
              "       [93, 68, 23, 99, 79],\n",
              "       [90, 47, 67, 82, 54]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def mat(n):\n",
        "  arr = np.zeros(n,dtype = int)\n",
        "  print(arr)\n",
        "  for i in range(n):\n",
        "    # a = []\n",
        "    for j in range(n):\n",
        "      arr[i][j] = random.randint(1,100)\n",
        "      # a.append(random.randint(1,100))\n",
        "    # arr.append(a)\n",
        "  return arr\n",
        "\n",
        "n = int(input(\"Enter : \"))\n",
        "print(mat(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "SFODsscYRT3L",
        "outputId": "3d330817-f9ac-44d2-bb5b-072a9ceeedd2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter : 5\n",
            "[0 0 0 0 0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e566edaa9e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-e566edaa9e10>\u001b[0m in \u001b[0;36mmat\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# a = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0;31m# a.append(random.randint(1,100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# arr.append(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(input(\"Enter the limit : \"))\n",
        "print(f\"The limit is n : {n}\")\n",
        "l=[]\n",
        "for i in range(2,n+1):\n",
        "    count = 0\n",
        "    for j in range(1,i+1):\n",
        "      if i%j == 0:\n",
        "        count = count+1\n",
        "    if count == 2:\n",
        "      l.append(i)\n",
        "print(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8BKrWVOSemX",
        "outputId": "ace6fc96-1df3-4433-f99c-16ec1e85948e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the limit : 200\n",
            "The limit is n : 200\n",
            "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH1ADHmGqNvX"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}